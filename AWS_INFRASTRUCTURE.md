**почніть читати з файла README.md**

## VPC та мережа
Я б створив окрему VPC з CIDR 10.0.0.0/16, щоб забезпечити повну ізоляцію від усього зайвого і мати повний контроль над трафіком. 

Зробив би 2 публічні та 2 приватні сабнети (приватні для подів і воркер нод) у двох різних AZ , наприклад eu-central-1a і eu-central-1b для високої доступності. Якщо одна зона ляже, інша буде праюцвати
Для публічних сабнетів зробив би IGW , а для приватних NAT Gateway в кожній AZ (сам NAT в паблік сабнеті, бо йому потрібен публічний айпі) Це щоб ноди і поди мали вихід в інтернет, але без прямого доступу ззовні.

## Security Groups
окрема для ALB (відкрити тільки 80/443 з інтернету) і для EKS нод (вхід від ALB, між подами і контроль плейн)
І для оптимізації VPC Endpoints для ECR і S3, щоб приватні поди тягнули образи без проходу через NAT, це зекономить на трафіку і зробить все швидше.

## EKS
Піднімаю EKS кластер: окремий для прод, і один спільний для dev та stage з ізоляцією через namespaces. Прод окремо, бо не дай бог тестовий деплой щось зламає в основному. Dev і stage разом — економія, але з quotas на ресурси, щоб не переїдали одне в одного.

## Managed Node Groups на EC2
Візьму інстанси t3.medium , їх вистачить для Node.js, вони не дорогі і прості. Якщо навантаження буде рости , додам нод.
Вмикаю логування control plane в CloudWatch. Так я зможу бачити всі дії, хто що робив та чому поди падають.
Додаю EKS add ons - обов'язково AWS Load Balancer Сontroller для інтеграції з ALB через інгресс, і Cluster аutoscaler для автоматичного скейлингу нод при навантаженні

## ECR
Створюю окремий репозиторій в ECR для кожного сервісу  (фронт, API, і по одному на мікросервіс). 

## lifecycle policy 
Поставлю щоб зберігались тільки останні 30 імейджів, старі видалятимуться автоматично. Зекономимо копієчку на цьому тому що не будемо зберігати не потрібні образи

## Балансування трафіку
Використовую один ALB для зовнішнього доступу. 
Routing за хостом - app.domain.com іде на фронт, api.domain.com іде на апі.

Для внутрішніх мікросервісів - Service типу ClusterIP, щоб було можливе спілкування в середині кластера
Інтеграція через інгрес в K8s з annotations для AWS LB Controller , так ALB створюється автоматично а не руками
HTTPS зроблю з сертифікатами з ACM та DNS - CNAME на ALB для app і api.

## DNS і TLS - Route 53 + ACM
Зроблю домен в Route 53, випущю сертифікат в ACM в тому ж регіоні де у нас ALB. Отримаємо автоматичне оновлення і просту інтеграцію з інгрес

## IAM доступи
Налаштую ролі з мінімальними правами , заюзаю least privilege принцип
Для CI/CD  - через OIDC для того щоб джоба могла брати тимчасові креденшали для пуша в ECR і деплоя в EKS , без статичних ключів буде безпечніше

Роль для нод - базові права для EKS роботи

Використовую IRSA для подів - якщо под бекенду юзає S3, то роль з s3:GetObject тільки для нього. AWS LB Controller отрмає права на створення ALB, а Cluster Autoscaler на скейлинг груп
Так ніхто не має зайвого, зменшуємо ризики

## Логи та моніторинг
Для логів поставлю AWS Fluent Bit як Daemonset в кластері, щоб все з stdout та stderr подів летіло в CloudWatch Logs.

Нааштування retention - 7 днів для dev/stage та 30 днів для prod. Знову зекономимо копієчку тим що не будемо тримати все вічно.
Для метрик -  вмикаю Container Insights в CloudWatch, щоб бачити CPU та память по подам, корисно для скейлингу мікросервісів.
Сповіщення - на 5xx в ALB, CPU >80% , часті рестарти подів -  через SNS на email або Slack, щоб швидко реагувати
